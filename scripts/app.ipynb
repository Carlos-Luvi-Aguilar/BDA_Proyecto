{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cea6d64",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AudioAnalysis\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"üîÑ INICIANDO COMPARACI√ìN PANDAS vs PYSPARK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Medir tiempo de carga de datos\n",
    "print(\"\\nüìä COMPARACI√ìN DE CARGA DE DATOS\")\n",
    "\n",
    "# Pandas\n",
    "start_time = time.time()\n",
    "df_pandas = pd.read_csv(\"resultados_voz.csv\")\n",
    "pandas_load_time = time.time() - start_time\n",
    "\n",
    "# PySpark\n",
    "start_time = time.time()\n",
    "df_spark = spark.read.csv(\"resultados_voz.csv\", header=True, inferSchema=True)\n",
    "# Forzar la carga de datos\n",
    "df_spark.count()  # Esta acci√≥n fuerza la lectura completa\n",
    "spark_load_time = time.time() - start_time\n",
    "\n",
    "print(f\"Pandas - Tiempo de carga: {pandas_load_time:.4f} segundos\")\n",
    "print(f\"PySpark - Tiempo de carga: {spark_load_time:.4f} segundos\")\n",
    "print(f\"Ratio Pandas/PySpark: {pandas_load_time/spark_load_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2f6fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nüìà COMPARACI√ìN DE OPERACIONES B√ÅSICAS\")\n",
    "\n",
    "# Operaciones en Pandas\n",
    "start_time = time.time()\n",
    "pandas_stats = df_pandas.describe()\n",
    "pandas_shape = df_pandas.shape\n",
    "pandas_null_count = df_pandas.isnull().sum()\n",
    "pandas_basic_ops_time = time.time() - start_time\n",
    "\n",
    "# Operaciones en PySpark\n",
    "start_time = time.time()\n",
    "spark_stats = df_spark.describe()\n",
    "spark_shape = (df_spark.count(), len(df_spark.columns))\n",
    "spark_null_count = df_spark.select([count(when(col(c).isNull(), c)).alias(c) for c in df_spark.columns])\n",
    "# Forzar ejecuci√≥n\n",
    "spark_stats.show()\n",
    "spark_null_count.show()\n",
    "spark_basic_ops_time = time.time() - start_time\n",
    "\n",
    "print(f\"Pandas - Tiempo operaciones b√°sicas: {pandas_basic_ops_time:.4f} segundos\")\n",
    "print(f\"PySpark - Tiempo operaciones b√°sicas: {spark_basic_ops_time:.4f} segundos\")\n",
    "print(f\"Ratio Pandas/PySpark: {pandas_basic_ops_time/spark_basic_ops_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10312cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîß COMPARACI√ìN DE TRANSFORMACIONES COMPLEJAS\")\n",
    "\n",
    "# Crear una copia para evitar modificar el original\n",
    "df_pandas_processed = df_pandas.copy()\n",
    "\n",
    "# Pandas: An√°lisis de calidad de audio (simulado)\n",
    "start_time = time.time()\n",
    "# Calcular m√©tricas adicionales\n",
    "df_pandas_processed['F0_normalized'] = (df_pandas_processed['F0_Hz'] - df_pandas_processed['F0_Hz'].mean()) / df_pandas_processed['F0_Hz'].std()\n",
    "df_pandas_processed['Jitter_Shimmer_ratio'] = df_pandas_processed['Jitter_porcentaje'] / df_pandas_processed['Shimmer_porcentaje']\n",
    "df_pandas_processed['calidad_categoria'] = np.where(df_pandas_processed['HNR_dB'] > 15, 'Buena', \n",
    "                                                   np.where(df_pandas_processed['HNR_dB'] > 10, 'Aceptable', 'Mala'))\n",
    "\n",
    "# Agrupaciones y agregaciones\n",
    "pandas_grouped = df_pandas_processed.groupby('calidad_categoria').agg({\n",
    "    'F0_Hz': ['mean', 'std', 'count'],\n",
    "    'HNR_dB': 'mean',\n",
    "    'Jitter_porcentaje': 'mean'\n",
    "})\n",
    "pandas_transform_time = time.time() - start_time\n",
    "\n",
    "# PySpark: Mismas operaciones\n",
    "start_time = time.time()\n",
    "# Calcular m√©tricas adicionales\n",
    "df_spark_processed = df_spark.withColumn(\n",
    "    'F0_normalized', \n",
    "    (col('F0_Hz') - mean('F0_Hz').over(window.Window.partitionBy())) / stddev('F0_Hz').over(window.Window.partitionBy())\n",
    ").withColumn(\n",
    "    'Jitter_Shimmer_ratio', \n",
    "    col('Jitter_porcentaje') / col('Shimmer_porcentaje')\n",
    ").withColumn(\n",
    "    'calidad_categoria',\n",
    "    when(col('HNR_dB') > 15, 'Buena')\n",
    "    .when(col('HNR_dB') > 10, 'Aceptable')\n",
    "    .otherwise('Mala')\n",
    ")\n",
    "\n",
    "# Agrupaciones y agregaciones\n",
    "from pyspark.sql import window\n",
    "spark_grouped = df_spark_processed.groupBy('calidad_categoria').agg(\n",
    "    mean('F0_Hz').alias('F0_mean'),\n",
    "    stddev('F0_Hz').alias('F0_std'),\n",
    "    count('F0_Hz').alias('count'),\n",
    "    mean('HNR_dB').alias('HNR_mean'),\n",
    "    mean('Jitter_porcentaje').alias('Jitter_mean')\n",
    ")\n",
    "# Forzar ejecuci√≥n\n",
    "spark_grouped.show()\n",
    "spark_transform_time = time.time() - start_time\n",
    "\n",
    "print(f\"Pandas - Tiempo transformaciones: {pandas_transform_time:.4f} segundos\")\n",
    "print(f\"PySpark - Tiempo transformaciones: {spark_transform_time:.4f} segundos\")\n",
    "print(f\"Ratio Pandas/PySpark: {pandas_transform_time/spark_transform_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb9ffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ COMPARACI√ìN DE FILTRADO Y ORDENAMIENTO\")\n",
    "\n",
    "# Pandas\n",
    "start_time = time.time()\n",
    "pandas_filtered = df_pandas[\n",
    "    (df_pandas['HNR_dB'] > 12) & \n",
    "    (df_pandas['Jitter_porcentaje'] < 2.0)\n",
    "].sort_values('F0_Hz', ascending=False)\n",
    "pandas_filter_sort_time = time.time() - start_time\n",
    "\n",
    "# PySpark\n",
    "start_time = time.time()\n",
    "spark_filtered = df_spark.filter(\n",
    "    (col('HNR_dB') > 12) & \n",
    "    (col('Jitter_porcentaje') < 2.0)\n",
    ").orderBy(col('F0_Hz').desc())\n",
    "# Forzar ejecuci√≥n\n",
    "spark_filtered.count()\n",
    "spark_filter_sort_time = time.time() - start_time\n",
    "\n",
    "print(f\"Pandas - Tiempo filtrado/ordenamiento: {pandas_filter_sort_time:.4f} segundos\")\n",
    "print(f\"PySpark - Tiempo filtrado/ordenamiento: {spark_filter_sort_time:.4f} segundos\")\n",
    "print(f\"Ratio Pandas/PySpark: {pandas_filter_sort_time/spark_filter_sort_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee774344",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä RESUMEN DE RESULTADOS COMPARATIVOS\")\n",
    "\n",
    "# Recolectar todos los tiempos\n",
    "operaciones = ['Carga de Datos', 'Operaciones B√°sicas', 'Transformaciones', 'Filtrado/Ordenamiento']\n",
    "tiempos_pandas = [pandas_load_time, pandas_basic_ops_time, pandas_transform_time, pandas_filter_sort_time]\n",
    "tiempos_spark = [spark_load_time, spark_basic_ops_time, spark_transform_time, spark_filter_sort_time]\n",
    "\n",
    "# Crear visualizaci√≥n comparativa\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Gr√°fico de barras comparativo\n",
    "x = np.arange(len(operaciones))\n",
    "width = 0.35\n",
    "\n",
    "ax1.bar(x - width/2, tiempos_pandas, width, label='Pandas', alpha=0.8, color='blue')\n",
    "ax1.bar(x + width/2, tiempos_spark, width, label='PySpark', alpha=0.8, color='red')\n",
    "ax1.set_xlabel('Operaciones')\n",
    "ax1.set_ylabel('Tiempo (segundos)')\n",
    "ax1.set_title('Comparaci√≥n de Tiempos: Pandas vs PySpark')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(operaciones, rotation=45)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Gr√°fico de ratios de velocidad\n",
    "ratios = [pandas_load_time/spark_load_time, \n",
    "          pandas_basic_ops_time/spark_basic_ops_time,\n",
    "          pandas_transform_time/spark_transform_time,\n",
    "          pandas_filter_sort_time/spark_filter_sort_time]\n",
    "\n",
    "colors = ['green' if ratio > 1 else 'red' for ratio in ratios]\n",
    "ax2.bar(operaciones, ratios, color=colors, alpha=0.7)\n",
    "ax2.axhline(y=1, color='black', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Operaciones')\n",
    "ax2.set_ylabel('Ratio (Pandas/PySpark)')\n",
    "ax2.set_title('Ratio de Velocidad: Pandas/PySpark\\n>1 = Pandas m√°s r√°pido\\n<1 = PySpark m√°s r√°pido')\n",
    "ax2.set_xticklabels(operaciones, rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar resumen num√©rico\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã RESUMEN NUM√âRICO DE COMPARACI√ìN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, op in enumerate(operaciones):\n",
    "    print(f\"\\n{op}:\")\n",
    "    print(f\"  Pandas: {tiempos_pandas[i]:.4f}s\")\n",
    "    print(f\"  PySpark: {tiempos_spark[i]:.4f}s\")\n",
    "    print(f\"  Ratio: {ratios[i]:.2f}x\")\n",
    "\n",
    "# Calcular promedios\n",
    "avg_ratio = np.mean(ratios)\n",
    "print(f\"\\nüìà RATIO PROMEDIO: {avg_ratio:.2f}x\")\n",
    "\n",
    "if avg_ratio > 1:\n",
    "    print(\"‚úÖ CONCLUSI√ìN: Pandas es m√°s r√°pido en promedio\")\n",
    "else:\n",
    "    print(\"‚úÖ CONCLUSI√ìN: PySpark es m√°s r√°pido en promedio\")\n",
    "\n",
    "# Cerrar sesi√≥n de Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87b7053",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüíæ AN√ÅLISIS DE USO DE MEMORIA\")\n",
    "\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024  # Convertir a MB\n",
    "\n",
    "# Medir memoria para Pandas\n",
    "mem_before_pandas = get_memory_usage()\n",
    "df_pandas_large = pd.concat([df_pandas] * 100, ignore_index=True)  # Simular dataset m√°s grande\n",
    "pandas_mem_usage = get_memory_usage() - mem_before_pandas\n",
    "\n",
    "# Reiniciar Spark para medici√≥n limpia\n",
    "spark.stop()\n",
    "spark = SparkSession.builder.appName(\"MemoryTest\").getOrCreate()\n",
    "\n",
    "# Medir memoria para PySpark\n",
    "mem_before_spark = get_memory_usage()\n",
    "df_spark_large = spark.createDataFrame(pd.concat([df_pandas] * 100, ignore_index=True))\n",
    "# Forzar caching para medici√≥n real\n",
    "df_spark_large.cache()\n",
    "df_spark_large.count()\n",
    "spark_mem_usage = get_memory_usage() - mem_before_spark\n",
    "\n",
    "print(f\"Pandas - Uso de memoria: {pandas_mem_usage:.2f} MB\")\n",
    "print(f\"PySpark - Uso de memoria: {spark_mem_usage:.2f} MB\")\n",
    "print(f\"Ratio de memoria Pandas/PySpark: {pandas_mem_usage/spark_mem_usage:.2f}x\")\n",
    "\n",
    "# Limpieza final\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "class_big_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
